# üåç Relevancia del Resumen Autom√°tico de Noticias

La **tarea de resumir texto autom√°ticamente**, especialmente noticias diarias, tiene gran relevancia para **investigadores** y **profesionales** de **Procesamiento de Lenguaje Natural (PLN)**, **IA** y **automatizaci√≥n de contenido**. Algunas razones clave son:

- **Avances en PLN**: Modelos como **T5** han revolucionado el PLN, permitiendo la generaci√≥n autom√°tica de res√∫menes coherentes y relevantes a gran escala.
  
- **Eficiencia y accesibilidad**: Res√∫menes autom√°ticos permiten a los usuarios consumir informaci√≥n r√°pida y eficientemente, manteni√©ndose al tanto sin leer art√≠culos largos.

- **Aplicaciones comerciales**: Sectores como **marketing de contenidos**, **medios de comunicaci√≥n** y **noticias personalizadas** se benefician de la capacidad de generar res√∫menes precisos y bien estructurados, mejorando la experiencia del usuario.

---

# üì∞ Utilidad del Resumen Autom√°tico de Noticias Diarias

### a. **Mejora de la experiencia del usuario en plataformas de noticias**  
Las **plataformas digitales de noticias** (como sitios web, aplicaciones m√≥viles o newsletters) pueden utilizar modelos como T5 para ofrecer **res√∫menes autom√°ticos** de los art√≠culos m√°s relevantes del d√≠a. Esto no solo mejora la **experiencia del usuario** al proporcionar **informaci√≥n condensada** y de f√°cil acceso, sino que tambi√©n permite a los usuarios **estar informados** sin tener que navegar a trav√©s de contenido extenso.

#### Ejemplo real:
Una **aplicaci√≥n de noticias** como **Flipboard** o **Google News** podr√≠a integrar un modelo de resumen autom√°tico que ofrezca a los usuarios un **resumen personalizado** y **breve** de las noticias m√°s relevantes, adaptado a sus intereses. De esta forma, la aplicaci√≥n podr√≠a presentar solo la **informaci√≥n esencial**, lo cual es especialmente √∫til para personas con poco tiempo.


### b. **Filtrado y personalizaci√≥n de contenido**  
Un **sistema de res√∫menes** puede ser utilizado para **filtrar contenido** de manera m√°s efectiva, presentando solo lo m√°s relevante para cada usuario. Esto ser√≠a especialmente √∫til para plataformas que ofrecen **noticias personalizadas**, como **Feedly** o **Pocket**. Los modelos de resumen no solo seleccionar√≠an las noticias m√°s relevantes, sino que tambi√©n las condensar√≠an, permitiendo a los usuarios consumirlas r√°pidamente.

#### Ejemplo real:
Un usuario que pregunta a su asistente virtual "¬øQu√© ha pasado en el mundo hoy?" podr√≠a recibir un **resumen breve y conciso** de las noticias m√°s importantes, generado autom√°ticamente por el modelo entrenado con datos de **DailyMail**. Esto mejora la **experiencia del usuario** al ofrecer informaci√≥n relevante de forma instant√°nea.

### c. **Asistentes Virtuales y Chatbots**  
Los **asistentes virtuales** (como **Siri**, **Alexa** o **Google Assistant**) podr√≠an integrar este tipo de modelos para ofrecer **res√∫menes autom√°ticos** de noticias diarias o temas espec√≠ficos a los usuarios. Estos res√∫menes podr√≠an entregarse **por voz** o en **formato de texto**, permitiendo que los usuarios se mantengan al tanto de las novedades sin tener que buscar activamente la informaci√≥n.

#### Ejemplo real:
Un servicio como **Pocket** o **Instapaper** podr√≠a ofrecer **res√∫menes autom√°ticos** de art√≠culos largos, optimizados seg√∫n los intereses de los usuarios. Imagina que un usuario lee a menudo sobre **tecnolog√≠a** o **pol√≠tica**. La aplicaci√≥n podr√≠a generar res√∫menes de los art√≠culos m√°s recientes sobre estos temas, presentados de forma concisa, para facilitar la lectura r√°pida.

---

## üõ†Ô∏è Modelo: T5-small

El modelo T5-small fue entrenado en un gran corpus de texto, lo que significa que tiene una comprensi√≥n general del lenguaje. Sin embargo, no est√° espec√≠ficamente preentrenado para res√∫menes de noticias como el dataset DailyMail. Es por eso que necesitas realizar un *fine-tuning* con este dataset espec√≠fico para que el modelo aprenda las peculiaridades de este tipo de texto.

### Tama√±o adecuado
T5-small es lo suficientemente peque√±o como para ser entrenado en hardware moderado, pero lo suficientemente grande como para ofrecer buenos resultados en tareas de resumen autom√°tico.

### Capacidad de fine-tuning
Puedes ajustar las √∫ltimas capas de T5-small y adaptar el modelo al dataset DailyMail para mejorar la calidad de los res√∫menes generados.

### Compatibilidad
El modelo est√° bien soportado en plataformas como TensorFlow y PyTorch, lo que facilita su implementaci√≥n y ajuste.

### Resumen
El modelo T5-small es adecuado para realizar *fine-tuning* en el dataset DailyMail para generar res√∫menes autom√°ticos de noticias. El dataset es lo suficientemente grande, pero puede requerir algunos pasos de preprocesamiento y limpieza. Si bien el modelo no est√° entrenado espec√≠ficamente para res√∫menes de noticias, su capacidad de generalizaci√≥n en tareas de texto lo hace una opci√≥n s√≥lida para este problema.

---

## üéØ Dataset: DailyMail

El dataset DailyMail est√° compuesto por art√≠culos de noticias y sus res√∫menes (targets) correspondientes. En t√©rminos generales, los datos est√°n balanceados en cuanto a la longitud de los art√≠culos y sus res√∫menes, ya que cada art√≠culo tiene un resumen de longitud variable.

El dataset DailyMail tiene un n√∫mero suficientemente grande de ejemplos (aproximadamente 300,000 pares de art√≠culos y res√∫menes), lo que deber√≠a ser adecuado para realizar un entrenamiento eficaz.

---

## üöÄ Conclusiones

Durante el entrenamiento del modelo <a href="https://huggingface.co/google-t5/t5-small">**T5-Small**</a> utilizando 100,000 pares de noticias y res√∫menes del conjunto de datos <a href="https://huggingface.co/datasets/abisee/cnn_dailymail">**CNN/DailyMail**</a>, se identificaron los mejores par√°metros y estrategias que llevaron a obtener m√©tricas ROUGE significativas. Las estad√≠sticas finales alcanzadas por el modelo son las siguientes:

- **ROUGE-1**: 0.3090  
- **ROUGE-2**: 0.1209  
- **ROUGE-L**: 0.2300  
- **ROUGE-LSum**: 0.2614  

### Razones de las mejores m√©tricas obtenidas

1. **Congelaci√≥n parcial del encoder**:  
   Al congelar las primeras 5 capas del encoder durante el entrenamiento, se redujo la cantidad de par√°metros ajustables. Esto permiti√≥ que el modelo aprovechara representaciones preentrenadas efectivas en las capas iniciales mientras se concentraba en ajustar las capas superiores para la tarea espec√≠fica de resumen. Este enfoque puede haber prevenido problemas de sobreajuste y permitido un mejor aprendizaje en un tiempo limitado.

2. **Tasa de aprendizaje moderada (`5e-5`) y programaci√≥n lineal**:  
   El uso de una tasa de aprendizaje relativamente baja, junto con un calentamiento inicial (`warmup_steps=1000`) y un programador lineal, contribuy√≥ a una convergencia m√°s estable. Esta configuraci√≥n ayud√≥ a evitar actualizaciones agresivas que podr√≠an haber llevado a oscilaciones o degradaci√≥n del desempe√±o.

3. **Regularizaci√≥n y estabilidad**:  
   - La **tasa de abandono (`dropout_rate=0.3`)** introdujo una robusta regularizaci√≥n que redujo el riesgo de sobreajuste al conjunto de entrenamiento.  
   - El **decaimiento de pesos (`weight_decay=0.01`)** contribuy√≥ a limitar la magnitud de los pesos del modelo, mejorando la generalizaci√≥n en el conjunto de evaluaci√≥n.

4. **Uso eficiente de recursos**:  
   - La acumulaci√≥n de gradientes (`gradient_accumulation_steps=2`) permiti√≥ entrenar con un mayor tama√±o de lote efectivo sin exceder los l√≠mites de memoria.  
   - El uso de punto flotante mixto (`fp16=True`) aceler√≥ el entrenamiento y mejor√≥ la eficiencia computacional.  

5. **M√©tricas ROUGE alineadas con la tarea**:  
   Las m√©tricas ROUGE obtenidas indican un desempe√±o s√≥lido considerando la capacidad del modelo (T5-Small) y el conjunto de datos empleado.  
   - **ROUGE-1** y **ROUGE-L**, que eval√∫an la coincidencia de n-gramas y la secuencia m√°s larga coincidente, destacan la capacidad del modelo para capturar informaci√≥n relevante y estructurar res√∫menes coherentes.  
   - El menor valor de **ROUGE-2**, m√°s exigente, refleja la dificultad inherente de capturar contextos precisos en res√∫menes m√°s concisos.

## üß† Resultados del modelo entrenado:

Tras observar las m√©tricas y cambiar los par√°metros que mejor funcionaban, entrenamos el modelo con 100.000 registros de noticias, afinando as√≠ en el resumen de √©stas.
Dada esta noticia:


  " Bitcoin surges past $100k for first time
  The price of Bitcoin `has for the first time broken past the $100,000 mark, hitting a new record high`.
  The value of the world's biggest cryptocurrency has been boosted by hopes US President-elect Donald Trump will adopt crypto-friendly policies.
  The milestone was reached hours after Trump said he would nominate former Securities and Exchange Commission (SEC) commissioner Paul Atkins to run the Wall Street regulator.
  Mr Atkins is seen as being far more pro-cryptocurrency than the current head of the SEC, Gary Gensler.
  The $100,000 milestone prompted celebrations from cryptocurrency fans around the world.
  Bitcoin's wildly fluctuating value has always attracted interest, with its backers reacting with delight when it has passed previous price thresholds - and defiance during its slumps.
  But this particular landmark has been especially keenly anticipated. `For weeks charts, memes and predictions have swirled around social media about when the price would hit the figure` thought to    be one of the holy grails of the crypto world.
  Millions of viewers even tuned in to online watch parties as the price hovered close to $100k.
  The value of a single bitcoin is one of the barometers of optimism in the cryptocurrency industry which is now estimated to be worth $3.3tn, according to analysis firm Coin Market Cap.
  Trump's election victory last month was the catalyst for the latest surge.
  The president-elect has vowed to make the US "the crypto capital of the planet" - a remarkable turnaround given as recently as 2021 he was calling Bitcoin a "scam."
  Also remarkable is just how Bitcoin's price has rocketed. `A valuation of $100k represents a 40% increase on election day in the U.S. and more than double the price it started the year at`.
  `But there's lots more to Bitcoin than the dizzying changes in its value`.
  From its enigmatic inventor to the bringing down of the so-called Crypto King, it's a story with many twists and turns, which has seen the making - and losing - of huge fortunes.
  So here's the BBC's list of the seven wildest moments - so far - in Bitcoin's tumultuous history. "


El resultado del modelo preentrenado sin fine-tuning es este:
```text
"has for the first time broken past the $100,000 mark, hitting a new record high. a valuation of $100k represents a 40% increase on election day in the U.S. and more than double the price it started the year at. but there's lots more to Bitcoin than the dizzying changes in its value. for weeks charts, memes and predictions have swirled around social media about when the price would hit the figure."
```
üîç Observamos c√≥mo se limita a copiar fragmentos de texto,subrayados en la noticia original, sin desarrollar nada ni dar coherencia.

Tras nuestro entrenamiento este es el resultado:
```text
"Bitcoin has for the first time broken past the $100,000 mark, hitting a new record high. The price of a single bitcoin is one of the barometers of optimism in the cryptocurrency industry. A valuation of $100k represents a 40% increase on election day in the U.S. and more than double the price it started the year at. But there's lots more to Bitcoin than the dizzying changes in its value."
```
Mostrando una clara mejor√≠a en fluidez y comprensi√≥n, adem√°s siendo capaz de trabajar el contexto de la noticia.

---

# üí° Reflexi√≥n final

El dise√±o de los hiperpar√°metros fue clave para maximizar el desempe√±o del modelo dado el tama√±o del conjunto de datos y las limitaciones inherentes de T5-Small. La combinaci√≥n de estrategias como la congelaci√≥n parcial, una tasa de aprendizaje moderada y una adecuada regularizaci√≥n, junto con la calidad del conjunto de datos CNN/DailyMail, permiti√≥ al modelo alcanzar m√©tricas competitivas en la tarea de generaci√≥n de res√∫menes. Estos resultados subrayan la importancia de ajustar cuidadosamente los hiperpar√°metros y de aprovechar t√©cnicas como la transferencia de aprendizaje para mejorar modelos compactos.
